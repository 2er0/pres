{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "base.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "EdAyi5JeCq2K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TensorFlow in an end-to-end example - Face Emotions Recognition\n",
        "\n",
        "@fhlug 23.4.2019 | by David Baumgartner\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WxSdTf5QaJJD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get the dataset/data\n",
        "\n",
        "- Emotion recognition\n",
        "  - Source: images with a resolution of 48x48px\n",
        "  - Target: 6 classes (original 7 classes)\n"
      ]
    },
    {
      "metadata": {
        "id": "9aqp8-jhABPJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Labels\n",
        "# Gitlab: https://gitlab.com/2er0/pres/raw/aeb84257634be88267f79b2cde7414d29216e38f/workshop/small_filtered_labels.npy\n",
        "# Drive: https://drive.google.com/file/d/1OFzaoNiY_E9TXhp-pZSQBCgJTjnKyEMq/view?usp=sharing\n",
        "!wget --no-check-certificate 'https://gitlab.com/2er0/pres/raw/aeb84257634be88267f79b2cde7414d29216e38f/workshop/small_filtered_labels.npy' -O small_filtered_labels.npy\n",
        "\n",
        "# Images\n",
        "# Gitlab: https://gitlab.com/2er0/pres/raw/aeb84257634be88267f79b2cde7414d29216e38f/workshop/small_filtered_images.npy\n",
        "# Drive: https://drive.google.com/file/d/1Jsoi8BysEbaiHijTFYmzS_6yJZqmXwUw/view?usp=sharing\n",
        "!wget --no-check-certificate 'https://gitlab.com/2er0/pres/raw/aeb84257634be88267f79b2cde7414d29216e38f/workshop/small_filtered_images.npy' -O small_filtered_images.npy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yKJN7O8QDDu4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Util\n",
        "\n",
        "- calculate the baseline (the result should be better than just guessing)"
      ]
    },
    {
      "metadata": {
        "id": "lZ--lt6hYUxb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "def calc_baseline(lab: np.ndarray) -> (float, int):\n",
        "    counters = collections.Counter(lab)\n",
        "    baseline = 0\n",
        "    label = -1\n",
        "    lab_len = len(lab)\n",
        "    for l, c in counters.items():\n",
        "        b = c / lab_len\n",
        "        if b > baseline:\n",
        "            baseline = b\n",
        "            label = l\n",
        "    return baseline * 100, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kciQdLhrDTJc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Load the dataset\n",
        "\n",
        "- load the sources and targets into the system memory or GPU memory\n",
        "- calculate the baseline for the loaded dataset for simple verification"
      ]
    },
    {
      "metadata": {
        "id": "5SVtkobAJPT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "\n",
        "# load images\n",
        "features = 'small_filtered_images.npy' # 48x48\n",
        "X = np.load(features)\n",
        "X = np.expand_dims(X, axis=3)\n",
        "\n",
        "# load labels\n",
        "labels = 'small_filtered_labels.npy' # 1\n",
        "Y = np.load(labels)\n",
        "baseline = calc_baseline(Y)\n",
        "Y = np.expand_dims(Y, axis=1)\n",
        "\n",
        "print(X.shape, Y.shape)\n",
        "print(baseline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "juDuyCQtDiNX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode the target\n",
        "\n",
        "- the target should be encoded so that every class can get a probability value"
      ]
    },
    {
      "metadata": {
        "id": "FqJZzPblKmF4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# encode labels -> OneHotEncoder\n",
        "\n",
        "raise RuntimeError(\"TODO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kByANYlgEbua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Split the data\n",
        "\n",
        "- split the dataset into separate datasets \n",
        "  - training (80%)\n",
        "  - testing (20%)\n",
        "  \n",
        "use the testing dataset __only__ for testing, never for or in the training lifecycle\n",
        "\n",
        "extract a __third dataset__ for validation during the training process, if the _tf.data.Dataset_-module is in use, then provide a manually created validation-set of data\n",
        "\n",
        "the final data splitting could look like: 70:10:20"
      ]
    },
    {
      "metadata": {
        "id": "Bu057ISKKD4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split dataset to training-, testing-set\n",
        "\n",
        "raise RuntimeError(\"TODO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MeCw1tp4FNwT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the input and output dimensions\n",
        "\n",
        "- what is the input\n",
        "- what is the output\n",
        "- what batch size would be nice\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "n3NZeH1pJJ9c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define input and out put shape for neural network\n",
        "\n",
        "raise RuntimeError(\"TODO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dab9ubqaFTlB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define a neural network model\n",
        "\n",
        "model definition depends on the data and the target"
      ]
    },
    {
      "metadata": {
        "id": "FGEv91iTXM3D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raise RuntimeError(\"TODO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rl4-_Wu-Gaww",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a model instance\n",
        "\n",
        "define which optimizer you want and how to calculate the error\n",
        "\n",
        "- params\n",
        "  - optimizer\n",
        "  - loss\n",
        "  - metrics\n",
        "  - ..."
      ]
    },
    {
      "metadata": {
        "id": "6mlIzqxmYLHI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create actual model for training\n",
        "\n",
        "raise RuntimeError(\"TODO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "czODri3CGha1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Start training the created model\n",
        "\n",
        "with the training define additional parameters like how many epochs of training\n",
        "\n",
        "- params\n",
        "  - validdation size\n",
        "  - epochs\n",
        "  - shuffle\n",
        "  - earlystopping\n",
        "  - patience\n",
        "  - save\n",
        "  - ..."
      ]
    },
    {
      "metadata": {
        "id": "GBYccpqQZKCF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# setup training\n",
        "\n",
        "raise RuntimeError(\"TODO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qiPT6GXgGnE2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot some metric information and the graph"
      ]
    },
    {
      "metadata": {
        "id": "d_jqgRz6k08M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output, Image, display, HTML\n",
        "import numpy as np    \n",
        "\n",
        "def strip_consts(graph_def, max_const_size=32):\n",
        "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
        "    strip_def = tf.GraphDef()\n",
        "    for n0 in graph_def.node:\n",
        "        n = strip_def.node.add() \n",
        "        n.MergeFrom(n0)\n",
        "        if n.op == 'Const':\n",
        "            tensor = n.attr['value'].tensor\n",
        "            size = len(tensor.tensor_content)\n",
        "            if size > max_const_size:\n",
        "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
        "    return strip_def\n",
        "\n",
        "def show_graph(graph_def, max_const_size=32):\n",
        "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
        "    if hasattr(graph_def, 'as_graph_def'):\n",
        "        graph_def = graph_def.as_graph_def()\n",
        "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
        "    code = \"\"\"\n",
        "        <script>\n",
        "          function load() {{\n",
        "            document.getElementById(\"{id}\").pbtxt = {data};\n",
        "          }}\n",
        "        </script>\n",
        "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
        "        <div style=\"height:600px\">\n",
        "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
        "        </div>\n",
        "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
        "\n",
        "    iframe = \"\"\"\n",
        "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
        "    \"\"\".format(code.replace('\"', '&quot;'))\n",
        "    display(HTML(iframe))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J7T0IpPHZr3m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#generate plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot history for accuracy\n",
        "plt.figure()\n",
        "plt.ioff()\n",
        "\n",
        "raise RuntimeError(\"TODO\")\n",
        "\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# plot history for loss\n",
        "plt.figure()\n",
        "plt.ioff()\n",
        "\n",
        "raise RuntimeError(\"TODO\")\n",
        "\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# show the current graph with TensorBoard\n",
        "show_graph(tf.get_default_graph().as_graph_def())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsoZYuW-JUyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load a model with Keras\n",
        "\n",
        "- not frozen\n",
        "- training can continue\n"
      ]
    },
    {
      "metadata": {
        "id": "9PNImy-NL05v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load a trained keras model\n",
        "\n",
        "raise RuntimeError(\"TODO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KV8EHTHzJ_yb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Face detection in images"
      ]
    },
    {
      "metadata": {
        "id": "enzPBy1sJxiA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# face detector\n",
        "# Gitlab: https://gitlab.com/2er0/pres/raw/aeb84257634be88267f79b2cde7414d29216e38f/workshop/haarcascade_frontalface_default.xml\n",
        "# Drive: https://drive.google.com/file/d/1xSUK0snZeeJ_e1ekRaI-8rYHm_MNeuwh\n",
        "!wget --no-check-certificate 'https://gitlab.com/2er0/pres/raw/aeb84257634be88267f79b2cde7414d29216e38f/workshop/haarcascade_frontalface_default.xml' -O haarcascade_frontalface_default.xml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jcZpcw-xMSfY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification\n",
        "\n",
        "1. find face(s) in an image\n",
        "- clip face(s) out of the image\n",
        "- predict a label to each of the face(s)\n",
        "- render the result as an image"
      ]
    },
    {
      "metadata": {
        "id": "0BZbCzu2b7B-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "face_cascade = cv.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "\n",
        "emoMap = {0: \"angry\",\n",
        "          1: \"fear\",\n",
        "          2: \"happy\",\n",
        "          3: \"sad\",\n",
        "          4: \"surprise\",\n",
        "          5: \"neutral\",\n",
        "          6: \"contempt\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ekEnzTBw-KZr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# detect face in image\n",
        "\n",
        "def detect_face_and_classify(img, m):\n",
        "  # convert to gray\n",
        "  raise RuntimeError(\"TODO\")\n",
        "  \n",
        "  # detect faces\n",
        "  faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "  \n",
        "  # emotiondetection\n",
        "  if len(faces) == 0:\n",
        "    return img\n",
        "  \n",
        "  face_features = []\n",
        "  x_y_pos = []\n",
        "  for face in faces:\n",
        "      (x, y, w, h) = face\n",
        "\n",
        "      # rescale cropboarder to be perfect square\n",
        "      if w > h:\n",
        "          diff = w - h\n",
        "          y = y - int(diff / 2)\n",
        "          h = h + int(((diff / 2) - (diff % 2)))\n",
        "      else:\n",
        "          diff = h - w\n",
        "          x = x - int(diff / 2)\n",
        "          w = w + int((diff / 2) - (diff % 2))\n",
        "\n",
        "      # draw crop rectangle\n",
        "      cv.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "      # crop image\n",
        "      c = gray[y:y + h, x:x + w]\n",
        "\n",
        "      # scale image for classification\n",
        "      raise RuntimeError(\"TODO\")\n",
        "\n",
        "      # hist equalisation\n",
        "      raise RuntimeError(\"TODO\")\n",
        "\n",
        "      # expand dimenstions\n",
        "      raise RuntimeError(\"TODO\")\n",
        "\n",
        "      face_features.append(c)\n",
        "      x_y_pos.append((x, y, w))\n",
        "\n",
        "  face_features = np.asarray(face_features)\n",
        "  # predict class\n",
        "  # class_predictions = m.predict_classes(face_features)\n",
        "  each_class_predictions = m.predict(face_features)\n",
        "  # show mapped class\n",
        "  for all_classes, (x, y, w) in zip(each_class_predictions, x_y_pos):\n",
        "      class_pred = int(encoder.inverse_transform([all_classes])[0])\n",
        "\n",
        "      cv.putText(img, emoMap[class_pred], (x + 4, y - 4),\n",
        "                 cv.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv.LINE_AA)\n",
        "  \n",
        "  return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m2x5nfBwomId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import precoded take_photo sample\n",
        "\n",
        "raise RuntimeError(\"TODO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JDDQZKORvLbs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run - collect photo and classify it\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "raise RuntimeError(\"TODO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fz7i473fMfoe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  # Freeze & Optimize Model (Production)"
      ]
    },
    {
      "metadata": {
        "id": "4z6yaI_o7UxN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://www.dlology.com/blog/how-to-convert-trained-keras-model-to-tensorflow-and-make-prediction/\n",
        "\n",
        "def freeze_session_1(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes the state of a session into a pruned computation graph.\n",
        "\n",
        "    Creates a new computation graph where variable nodes are replaced by\n",
        "    constants taking their current value in the session. The new graph will be\n",
        "    pruned so subgraphs that are not necessary to compute the requested\n",
        "    outputs are removed.\n",
        "    @param session The TensorFlow session to be frozen.\n",
        "    @param keep_var_names A list of variable names that should not be frozen,\n",
        "                          or None to freeze all the variables in the graph.\n",
        "    @param output_names Names of the relevant graph outputs.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    from tensorflow.graph_util import convert_variables_to_constants\n",
        "    graph = session.graph\n",
        "    with graph.as_default():\n",
        "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
        "        output_names = output_names or []\n",
        "        output_names += [v.op.name for v in tf.global_variables()]\n",
        "        print(output_names)\n",
        "        input_graph_def = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in input_graph_def.node:\n",
        "                node.device = \"\"\n",
        "        frozen_graph = convert_variables_to_constants(session, input_graph_def, \n",
        "                                                      output_names, freeze_var_names)\n",
        "        return frozen_graph\n",
        "      \n",
        "def freeze_session_2(model_dir, output_node_names):\n",
        "    \"\"\"Extract the sub graph defined by the output nodes and convert \n",
        "    all its variables into constant \n",
        "    Args:\n",
        "        model_dir: the root folder containing the checkpoint state file\n",
        "        output_node_names: a string, containing all the output node's names, \n",
        "                            comma separated\n",
        "    \"\"\"\n",
        "    if not tf.gfile.Exists(model_dir):\n",
        "        raise AssertionError(\n",
        "            \"Export directory doesn't exists. Please specify an export \"\n",
        "            \"directory: %s\" % model_dir)\n",
        "\n",
        "    if not output_node_names:\n",
        "        print(\"You need to supply the name of a node to --output_node_names.\")\n",
        "        return -1\n",
        "\n",
        "    # We retrieve our checkpoint fullpath\n",
        "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
        "    input_checkpoint = checkpoint.model_checkpoint_path\n",
        "    \n",
        "    # We precise the file fullname of our freezed graph\n",
        "    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
        "    output_graph = absolute_model_dir + \"/frozen_model.pb\"\n",
        "\n",
        "    # We clear devices to allow TensorFlow to control on which device it will load operations\n",
        "    clear_devices = True\n",
        "\n",
        "    # We start a session using a temporary fresh Graph\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        # We import the meta graph in the current default Graph\n",
        "        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
        "\n",
        "        # We restore the weights\n",
        "        saver.restore(sess, input_checkpoint)\n",
        "\n",
        "        # We use a built-in TF helper to export variables to constants\n",
        "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
        "            sess, # The session is used to retrieve the weights\n",
        "            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
        "            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n",
        "        ) \n",
        "\n",
        "        # Finally we serialize and dump the output graph to the filesystem\n",
        "        with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
        "            f.write(output_graph_def.SerializeToString())\n",
        "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
        "\n",
        "    return output_graph_def"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EnlfEgaflTdD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Version 1: freeze-session"
      ]
    },
    {
      "metadata": {
        "id": "-StP4Mnr7WJJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.python.tools import optimize_for_inference_lib\n",
        "from tensorflow.tools.graph_transforms import TransformGraph\n",
        "\n",
        "# Create, compile and train model...\n",
        "print('Input: ', [i.op.name for i in model.inputs], model.input)\n",
        "print('Output: ', [o.op.name for o in model.outputs], model.output)\n",
        "\n",
        "K.set_learning_phase(0)\n",
        "k_sess = K.get_session()\n",
        "\n",
        "model_path = './out2'\n",
        "model_name = 'model'\n",
        "input_node_name = [i.op.name for i in model.inputs][0]\n",
        "output_node_name = [o.op.name for o in model.outputs][0]\n",
        "\n",
        "frozen_graph = freeze_session_1(k_sess, output_names=[out.op.name for out in model.outputs])\n",
        "tf.train.write_graph(frozen_graph, model_path, f'frozen_{model_name}.pb', as_text=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eXqEG36elaA0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Version 2: optimize for inference"
      ]
    },
    {
      "metadata": {
        "id": "_bCmh27ckIn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "graph_def = optimize_for_inference_lib.optimize_for_inference(k_sess.graph.as_graph_def(), [input_node_name], [output_node_name], tf.float32.as_datatype_enum, True)\n",
        "graph_def = TransformGraph(graph_def, [input_node_name], [output_node_name], ['fold_constants', 'sort_by_execution_order'])\n",
        "with tf.gfile.GFile(f'./{model_path}/opti_{model_name}.pb', 'wb') as f:\n",
        "  f.write(graph_def.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "THkx_ZDBlgSU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Checkpoints"
      ]
    },
    {
      "metadata": {
        "id": "z6eyGeLbYVra",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.train.Saver().save(k_sess, f'{model_path}/{model_name}.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QftBcFR2l8qc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_graph(graph_def)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Wqda_8kKnSv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Face detection in Images with optimized model\n",
        "\n",
        "based on classifier within OpenCV or native TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "QhqO0qETKiBG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Util 2"
      ]
    },
    {
      "metadata": {
        "id": "7iD0V66AaL7Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def load_graph(frozen_graph_filename):\n",
        "  # We load the protobuf file from the disk and parse it to retrieve the \n",
        "  # unserialized graph_def\n",
        "  with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
        "      graph_def = tf.GraphDef()\n",
        "      graph_def.ParseFromString(f.read())\n",
        "\n",
        "  # Then, we import the graph_def into a new Graph and returns it \n",
        "  with tf.Graph().as_default() as graph:\n",
        "      # The name var will prefix every op/nodes in your graph\n",
        "      # Since we load everything in a new graph, this is not needed\n",
        "      tf.import_graph_def(graph_def, name=\"prefix\")\n",
        "  return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SPY7vkA1ZO2b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Load a native TensorFlow graph\n",
        "\n",
        "A TenserFlow graph loaded with the provided API or with a third party library\n",
        "\n",
        "- TensorFlow API (C++, Java, Go, Python, ...)\n",
        "- OpenCV has load function (works not 100%)\n"
      ]
    },
    {
      "metadata": {
        "id": "lwiMZfh-OGh3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# detect face in image\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "predict_with_cv = False\n",
        "frozen_name = 'todo'\n",
        "raise RuntimeError(\"TODO: set model path for loading load\")\n",
        "\n",
        "# works only if only one model is in the current session\n",
        "if predict_with_cv:\n",
        "  cv_model = cv.dnn.readNetFromTensorflow(frozen_name)\n",
        "\n",
        "else:\n",
        "  graph = load_graph(frozen_name)\n",
        "  layer_names = [o.name for o in graph.get_operations()]\n",
        "  print(list(filter(lambda x : x.count('nput')>0, layer_names)))\n",
        "  print(list(filter(lambda x : x.count('oftmax')>0, layer_names)))\n",
        "\n",
        "  # We access the input and output nodes \n",
        "  tensor_input = graph.get_tensor_by_name(f'prefix/{input_node_name}:0')\n",
        "  tensor_output = graph.get_tensor_by_name(f'prefix/{output_node_name}:0')\n",
        "  sess = tf.Session(graph=graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l1G8ErkCmM2e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_graph(tf.get_default_graph().as_graph_def())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FPNBZB-HfxVq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Update Classification\n",
        "\n",
        "changing the classification to the frozen and optimized model"
      ]
    },
    {
      "metadata": {
        "id": "itid-ozX5blm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def detect_face_and_classify_freezed(img):\n",
        "  # convert to gray\n",
        "  raise RuntimeError(\"TODO\")\n",
        "  \n",
        "  # detect faces\n",
        "  faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "  \n",
        "  # emotiondetection\n",
        "  if len(faces) == 0:\n",
        "    return img\n",
        "  \n",
        "  face_features = []\n",
        "  x_y_pos = []\n",
        "  for face in faces:\n",
        "      (x, y, w, h) = face\n",
        "\n",
        "      # rescale cropboarder to be perfect square\n",
        "      if w > h:\n",
        "          diff = w - h\n",
        "          y = y - int(diff / 2)\n",
        "          h = h + int(((diff / 2) - (diff % 2)))\n",
        "      else:\n",
        "          diff = h - w\n",
        "          x = x - int(diff / 2)\n",
        "          w = w + int((diff / 2) - (diff % 2))\n",
        "\n",
        "      # draw crop rectangle\n",
        "      cv.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "      # crop image\n",
        "      c = gray[y:y + h, x:x + w]\n",
        "\n",
        "      # scale image for classification\n",
        "      raise RuntimeError(\"TODO\")\n",
        "\n",
        "      # hist equalisation\n",
        "      raise RuntimeError(\"TODO\")\n",
        "\n",
        "      # expand dimenstions\n",
        "      raise RuntimeError(\"TODO\")\n",
        "\n",
        "      face_features.append(c)\n",
        "      x_y_pos.append((x, y, w))\n",
        "\n",
        "  face_features = np.asarray(face_features)\n",
        "  \n",
        "  # predict class\n",
        "  if predict_with_cv:\n",
        "    cv_model.setInput(cv.dnn.blobFromImages(face_features, size=(48, 48), swapRB=True, crop=False))\n",
        "    each_class_predictions = cv_model.forward()\n",
        "  else:\n",
        "    each_class_predictions = sess.run(tensor_output, {tensor_input: face_features})\n",
        "    \n",
        "  # show mapped class\n",
        "  for all_classes, (x, y, w) in zip(each_class_predictions, x_y_pos):\n",
        "      class_pred = int(encoder.inverse_transform([all_classes])[0])\n",
        "\n",
        "      cv.putText(img, emoMap[class_pred], (x + 4, y - 4),\n",
        "                 cv.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv.LINE_AA)\n",
        "  \n",
        "  return img, each_class_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRhvDjVeK5Wm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get a picture and classify it, part 2"
      ]
    },
    {
      "metadata": {
        "id": "rPruHEaEA1f7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# use frozen model\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "raise RuntimeError(\"TODO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "64ByYShsgLNd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Finish up\n",
        "\n",
        "how much space are the models now using\n"
      ]
    },
    {
      "metadata": {
        "id": "ng0gRt_TmHBe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# The folder containing files.\n",
        "directory = \"todo\"\n",
        "raise RuntimeError(\"TODO: set dir path to check\")\n",
        "\n",
        "# Get all files.\n",
        "items = os.listdir(directory)\n",
        "\n",
        "# Loop and add files to list.\n",
        "pairs = []\n",
        "for file in items:\n",
        "\n",
        "    # Use join to get full file path.\n",
        "    location = os.path.join(directory, file)\n",
        "\n",
        "    # Get size and add to list of tuples.\n",
        "    size = os.path.getsize(location)\n",
        "    pairs.append((size, file))\n",
        "\n",
        "# Sort list of tuples by the first element, size.\n",
        "pairs.sort(key=lambda s: s[0])\n",
        "\n",
        "# Display pairs.\n",
        "for pair in pairs:\n",
        "    print(pair)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L2lQj2rVgbSK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Thanks for every support and feedback\n",
        "\n",
        "- Andi Haghofer\n",
        "- Daniel Knittl-Frank\n",
        "- Rainhard Findling\n",
        "- Gerald Zwettler\n",
        "- ...\n"
      ]
    }
  ]
}